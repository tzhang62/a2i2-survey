# Switch to Hugging Face API - Implementation Guide

## Overview
This guide shows exactly what to change in your code to use Hugging Face API instead of local IQL inference.

## Benefits After Switching:
- ‚ö° **10x faster** Session 2 responses
- üí∞ **Stays on free tier** (Render + HF both free)
- ü™∂ **90% lighter** backend (no PyTorch/transformers)
- üöÄ **Better performance** (GPU inference on HF)

---

## Step 1: Upload Model to Hugging Face

```bash
cd /Users/tzhang/projects/a2i2_survey/a2i2_chatbot/backend

# Install Hugging Face CLI
pip install huggingface_hub

# Login (you'll need to create HF account first)
huggingface-cli login

# Run upload script
python upload_iql_to_hf.py
```

This will:
1. Package your IQL model
2. Upload to Hugging Face
3. Give you a model URL like: `YOUR_USERNAME/iql-fire-rescue`

---

## Step 2: Get Hugging Face Token

1. Go to https://huggingface.co/settings/tokens
2. Click "New token"
3. Name: `a2i2-survey-api`
4. Type: **Read** (not Write)
5. Copy the token (starts with `hf_...`)

---

## Step 3: Update Render Environment Variables

Add to Render:
```
HUGGINGFACE_MODEL_ID = YOUR_USERNAME/iql-fire-rescue
HUGGINGFACE_TOKEN = hf_your_token_here
```

---

## Step 4: Update server.py

### Change 1: Replace IQL imports

**Find this (around line 638-680):**
```python
# ============================================================================
# Initialize IQL System
# ============================================================================

def find_first_existing(*paths: Path) -> Path:
    """Find first existing path"""
    for p in paths:
        if p.exists():
            return p
    return None

def initialize_iql():
    """Initialize IQL system"""
    # ... long initialization code ...
```

**Replace with:**
```python
# ============================================================================
# Initialize IQL System (Hugging Face API)
# ============================================================================

from iql_hf_api import get_iql_hf

def initialize_iql():
    """Initialize IQL system using Hugging Face API"""
    try:
        iql_system = get_iql_hf()
        print("[IQL] System initialized successfully (using Hugging Face API)")
        return iql_system
    except Exception as e:
        print(f"[ERROR] IQL initialization failed: {e}")
        traceback.print_exc()
        return None
```

### Change 2: Update IQL calls

**Find this (around line 850-880 in send_message):**
```python
if character and iql_system:
    # Use IQL to select policy
    best_policy, qvals = iql_system.select_policy(history, n_last=N_LAST_RESIDENT)
```

**Replace with:**
```python
if character and iql_system:
    # Use IQL (via Hugging Face API) to select policy
    best_policy, qvals = iql_system.select_policy(history, character=character, n_last=N_LAST_RESIDENT)
```

That's it! The API is already designed to be a drop-in replacement.

---

## Step 5: Simplify requirements.txt

### Old requirements.txt:
```txt
fastapi==0.104.1
uvicorn==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
torch==2.5.1
transformers==4.36.0
sentence-transformers==2.3.1
tokenizers==0.15.0
requests==2.31.0
python-dotenv==1.0.0
numpy==1.26.2
scikit-learn==1.3.2
```

### New requirements.txt (MUCH lighter!):
```txt
fastapi==0.104.1
uvicorn==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
requests==2.31.0
python-dotenv==1.0.0
```

Removed:
- ‚ùå torch (800MB!)
- ‚ùå transformers (200MB)
- ‚ùå sentence-transformers (100MB)
- ‚ùå tokenizers
- ‚ùå numpy
- ‚ùå scikit-learn

**Total savings: ~1.1GB of dependencies!** üéâ

---

## Step 6: Add New File to Git

```bash
cd /Users/tzhang/projects/a2i2_survey

# Add the new HF API wrapper
git add a2i2_chatbot/backend/iql_hf_api.py

# Commit everything
git commit -m "Switch to Hugging Face API for IQL inference"

# Push
git push origin main
```

---

## Step 7: Redeploy on Render

1. Go to Render dashboard
2. Click "Manual Deploy" ‚Üí "Deploy latest commit"
3. Watch logs - should be MUCH faster build (no PyTorch!)
4. Wait for "Deploy live"

---

## Step 8: Test It!

1. Go to your Netlify URL
2. Complete survey
3. Start Session 2 (role-play conversation)
4. **Should be FAST now!** ‚ö°

---

## Troubleshooting

### "Model is loading" (first request)
- **Normal!** First API call takes 20-30 seconds as HF loads model
- Subsequent calls are instant
- Model stays loaded for ~15 minutes

### "API rate limit"
- Free tier: 30,000 requests/month
- For research, this is plenty
- Can upgrade to HF Pro ($9/mo) for unlimited

### "Token invalid"
- Check token is correct in Render environment
- Make sure it's a "Read" token
- Try regenerating the token

### Fallback behavior
- If HF API fails, uses simple rule-based fallback
- Still works, just not optimally
- Check Render logs for errors

---

## Performance Comparison

| Metric | Before (Local) | After (HF API) |
|--------|---------------|----------------|
| Session 2 response time | 15-30 seconds | 2-5 seconds |
| Build time on Render | 10-15 minutes | 2-3 minutes |
| Memory usage | 500MB+ | 100MB |
| Render tier needed | Starter ($7/mo) | Free ($0) |
| Monthly cost | $7+ | $0 |

---

## Next Steps

Once working:
1. ‚úÖ Test all 6 conversations
2. ‚úÖ Verify IQL policies are being selected
3. ‚úÖ Check Render logs for any errors
4. ‚úÖ Monitor HF API usage

---

## Rollback Plan

If something goes wrong:
1. Revert requirements.txt to include torch, etc.
2. Remove `from iql_hf_api import get_iql_hf`
3. Restore old `initialize_iql()` function
4. Redeploy

But it should work perfectly! üöÄ

---

**Questions?** Check:
- Render logs: Errors during startup
- Browser console: Frontend errors
- HF model page: API status

Good luck! Your survey should be much faster now! ‚ö°

